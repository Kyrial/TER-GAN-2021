{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"3_-_Classification_de_donnees_textuelles_solutionsintegrees.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GlieJPd_p9p-"},"source":["<IMG SRC=\"http://www.lirmm.fr/~poncelet/EGC2021/smallbandeau_EGC2021.png\" align=\"center\" >"]},{"cell_type":"markdown","metadata":{"id":"ZuSVeGiup9qD"},"source":["<H1> Classification de données textuelles </H1>\n","\n","\n","Lors de l'étape d'ingénierie de données textuelles nous avons vu que diverses opérations pouvaient être appliquées sur les textes et qu'au final il est possible d'obtenir des textes simplifiés. Nous allons, à présent, étudier comment faire de la classification à partir de données textuelles et comment convertir les textes en vecteurs pour pouvoir faire de la classification. \n"]},{"cell_type":"markdown","metadata":{"id":"-jU5FWA_N1Rc"},"source":["## **Installation**"]},{"cell_type":"markdown","metadata":{"id":"9ZyEeUSVp9qD"},"source":["\n","\n","\n","Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n","\n","*! pip install nom_librairie*  \n","\n","**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n","\n","**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utilisées lors de la présentation d'une fonction dans une cellule sont ré-importées de manière à indiquer d'où elles viennent et ainsi connaîter d'où vient la fonction afin de vous faciliter la réutilisation dans un autre projet.\n"," "]},{"cell_type":"code","metadata":{"id":"Yjse4WxDp9qE"},"source":["# utiliser cette cellule pour installer les librairies manquantes\n","# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n","# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n","# recommencer tant que toutes les librairies ne sont pas installées ...\n","\n","\n","#!pip install ..\n","\n","# ne pas oublier de relancer le kernel du notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHJbZW6sp9qE"},"source":["# Importation des différentes librairies, classes et fonctions utilespour le notebook\n","\n","#Sickit learn met régulièrement à jour des versions et \n","#indique des futurs warnings. \n","#ces deux lignes permettent de ne pas les afficher.\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","\n","# librairies générales\n","import pandas as pd\n","import re\n","from tabulate import tabulate\n","import time\n","import numpy as np\n","import pickle\n","import string\n","import base64\n","\n","# librairie affichage\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# librairies scikit learn\n","import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.base import TransformerMixin\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","\n","# librairies des classifiers utilisés\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# librairies NLTK\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer \n","from nltk.corpus import stopwords\n","from nltk import word_tokenize \n","\n"," \n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = set(stopwords.words('english')) \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ckau9gB7p9qF"},"source":["Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."]},{"cell_type":"code","metadata":{"id":"pF7o_crzp9qF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611163928935,"user_tz":-60,"elapsed":395433,"user":{"displayName":"Pascal Poncelet","photoUrl":"","userId":"12593690340642999315"}},"outputId":"60d9a78d-da3f-43ca-c986-861635ee3c5c"},"source":["# pour monter son drive Google Drive local\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pH_BlA48p9qG"},"source":["Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive : "]},{"cell_type":"code","metadata":{"id":"sh63Jj2Qp9qG"},"source":["%cd /content/gdrive/My Drive/Colab Notebooks/EGC_Ecole2021\n","#\n","# pour une utilisation sur une machine locale changer le chemin ci-dessous et décommenter la ligne\n","#%cd ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdzwZpbjh10e","colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"status":"error","timestamp":1617263317701,"user_tz":-120,"elapsed":1032,"user":{"displayName":"mael bono","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGV9Sg8JMsm3WGJR2eOuaYsc7c0cqi_B2HrIP8tw=s64","userId":"07491905652297096970"}},"outputId":"6c612e0d-9817-48e5-c97a-15579867eb6c"},"source":["# fonctions utilities (affichage, confusion, etc.)\n","from MyNLPUtilities import *\n","import SolClassificationDonneesTextuelles"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c5da789a0986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fonctions utilities (affichage, confusion, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMyNLPUtilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSolClassificationDonneesTextuelles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MyNLPUtilities'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"RbvJ2VsTN-j0"},"source":["## **Vectorisation**"]},{"cell_type":"markdown","metadata":{"id":"q_-hGReKp9qG"},"source":["\n","\n","L'objectif de la vectorisation est de transformer les documents en vecteurs. Il existe deux approches principales : \n","1. L'approche sac de mots dans laquelle il n'y a aucun ordre dans les termes utilisés et qui ne tient compte que du nombre d'occurrences des termes\n","1. l'approche basée sur TF_IDF qui ne tient pas non plus compte de l'ordre des termes mais qui pondère les valeurs grâce à TF_IDF au lieu de la fréquence des termes.\n","\n","**Remarque :** même si l'ordre des mots n'est pas pris en compte dans ces approches, les n-grammes peuvent partiellement servir à pallier ce problème. "]},{"cell_type":"markdown","metadata":{"id":"DNlm7HChODgi"},"source":["### **L'approche Sac de Mots (*Bag of Words*)**"]},{"cell_type":"markdown","metadata":{"id":"PdiCsgM6p9qH"},"source":["\n","\n","\n","La manière la plus simple de mettre sous la forme de vecteur (*vectorisation*) est d'utiliser les Bag of Words (BOW). Il s'agit, à partir d'une liste de mots (vocabulaire) de compter le nombre d'apparitions du mot du vocabulaire dans le document.  \n","\n","Cette opération se fait par :\n","1. Création d'une instance de la classe CountVectorizer.\n","1. Appel de la fonction fit() pour apprendre le vocabulaire.\n","1. Appel de la fonction transform() sur un ou plusieurs documents afin de les encoder dans le vecteur.  \n","\n","La classe CountVectorizer permet également de faire un ensemble de pré-traitement sur un document : mise en minuscule, suppression des stop words (mots vides), suppression des ponctuations ... mais elle ne peut pas lemmatiser ou rechercher les racines des termes. \n","\n","Les principaux paramètres utiles sont les suivants :  \n","1. *lowercase* booléen pour mettre en minuscule le document (défaut=True).\n","1. *token_pattern* pour éliminer des mots trop petits (défaut=None).\n","1. *stopwords* pour éliminer les stopwords du document (défaut=None).\n","1. *analyzer* pour préciser si l'on travaille avec des mots ou des caractères ou appliquer une fonction de pré-traitement (défaut=’word’).\n","1. *ngram_range* pour pouvoir utiliser des n-grammes de mots ou de caractères en fonction de la valeur d'*analyzer* (défaut=(1, 1), i.e. on ne considère qu'un mot).\n","1. *max_df* pour ignorer les termes qui ont une fréquence de document strictement supérieure à un seuil donné (termes trop fréquents) (défaut=1.0).\n","1. *min_df* pour ignorer les termes qui ont une fréquence de document (présence en % de documents) strictement inférieure à un seuil donné (termes peu fréquents) (défaut=1).\n","1. *max_features* pour limiter le nombre de caractéristiques (*features*) que le vecteur doit contenir (défaut : None).\n","\n","\n","Pour avoir plus d'information sur CountVectorizer, voir https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html  \n","\n","Nous décrivons par la suite les différents paramètres."]},{"cell_type":"code","metadata":{"id":"M1sNKuQcp9qH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611163931078,"user_tz":-60,"elapsed":397525,"user":{"displayName":"Pascal Poncelet","photoUrl":"","userId":"12593690340642999315"}},"outputId":"39b5e3b6-914b-4bef-d86d-70b86e6a7c40"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","#premier exemple sans paramètre\n","texte = [\"This is a simple EXAMPLE ! of CountVectorizer for creating a vector\"]\n","\n","print (\"document initial \",texte,'\\n')\n","# par defaut conversion en minuscule\n","vectorizer = CountVectorizer()\n","\n","# creation du vocabulaire\n","vectorizer.fit(texte)\n","# encodage du document\n","vector = vectorizer.transform(texte)\n","\n","# la liste des différents features\n","print (\"Les différents features sont\",vectorizer.get_feature_names(),' ... à noter tout est converti en minuscule\\n')\n","\n","# Contenu du vocabulaire\n","print (\"Vocabulaire : \")\n","print(vectorizer.vocabulary_)\n","\n","\n","# affichage de la taille du vecteur de sortie\n","\n","print (\"\\nTaille du vecteur :\\n\",vector.shape,'\\n')\n","\n","print (\"Conversion en mettant lowercase=False\")\n","vectorizer = CountVectorizer(lowercase=False)\n","# creation du vocabulaire\n","vectorizer.fit(texte)\n","\n","# la liste des différents features\n","print (\"Les différents features sont\",vectorizer.get_feature_names(),' ... à noter les majuscules sont conservées\\n')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["document initial  ['This is a simple EXAMPLE ! of CountVectorizer for creating a vector'] \n","\n","Les différents features sont ['countvectorizer', 'creating', 'example', 'for', 'is', 'of', 'simple', 'this', 'vector']  ... à noter tout est converti en minuscule\n","\n","Vocabulaire : \n","{'this': 7, 'is': 4, 'simple': 6, 'example': 2, 'of': 5, 'countvectorizer': 0, 'for': 3, 'creating': 1, 'vector': 8}\n","\n","Taille du vecteur :\n"," (1, 9) \n","\n","Conversion en mettant lowercase=False\n","Les différents features sont ['CountVectorizer', 'EXAMPLE', 'This', 'creating', 'for', 'is', 'of', 'simple', 'vector']  ... à noter les majuscules sont conservées\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IwPqWqufp9qI"},"source":["Il est possible de combiner *fit* et *transform* comme le montre l'exemple suivant où nous créons également un dataframe pour afficher le vecteur résultat."]},{"cell_type":"code","metadata":{"id":"TrEJEn6_p9qI"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","texte = [\"This is an example, ! of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","vectorizer = CountVectorizer()\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdns1yidp9qJ"},"source":["**token_pattern**  \n","\n","*token_pattern* peut être utilisé pour filtrer uniquement les mots qui font une certaine taille. Elle est, par exemple, fort utile pour supprimer les termes composés d'un seul caractère. Pour cela, il faut préciser une expression régulière."]},{"cell_type":"code","metadata":{"id":"WEXfcZQyp9qJ"},"source":["texte = [\"This is an example, ! of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","print (\"Le vocabulaire ne contient que des mots qui ont plus de trois caractères :\")\n","vectorizer = CountVectorizer(token_pattern=r'\\w{3,}')  \n","\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apQRIVAsp9qJ"},"source":["**stop_words**  \n","\n","*stop_words* permet de supprimer du vocabulaire les mots qui appartiennent aux stopwords d'une langue (e.g. stopwords='english'). Il se base sur une liste de stopwords définie. Il est également possible de préciser sa propre liste de stopwords. \n"]},{"cell_type":"code","metadata":{"id":"sHE3eMXSp9qK"},"source":["texte = [\"This is an example, ! of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","print (\"Le vocabulaire ne contient que des mots qui ne sont pas des stopwords anglais :\")\n","vectorizer = CountVectorizer(stop_words='english')  \n","\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)\n","\n","print (\"Le vocabulaire ne contient que des mots qui ne sont pas dans une liste spécifiée de stopwords (example, vector, creating) : \")\n","vectorizer = CountVectorizer(stop_words=['example','vector','creating'])  \n","\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1D8IzlHp9qK"},"source":["**Les n-grammes**  \n","Il est possible de préciser que les features sont composés de n-grammes à l'aide du paramètre *ngram_range*. Ce dernier spécifie l'intervalle de valeurs possibles. Par exemple *ngram_range=(1, 2)* permettra d'obtenir des n-grammes de taille 1 et 2 mots, *ngram_range=(1, 3)* des n-grammes de 1, 2 et 3 mots, *ngram_range=(3, 3)* des n-grammes de 3 mots, etc.  \n","\n","Par défaut, il s'agit de n-grammes de mots, pour avoir des n-grammes de caractères, le paramètre *analyzer* doit être initialisé avec *analyzer='char'*. "]},{"cell_type":"code","metadata":{"id":"Pr2B7c8Jp9qL"},"source":["texte = [\"This is an example of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","print (\"n-grammes de mots de taille 1\")\n","vectorizer = CountVectorizer(ngram_range=(1,1))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","print (\"\\nn-grammes de mots de taille 1 et 2\")\n","vectorizer = CountVectorizer(ngram_range=(1,2))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","print (\"\\nn-grammes de mots de taille 2 et 3\")\n","vectorizer = CountVectorizer(ngram_range=(2,3))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","print (\"\\nn-grammes de mots de taille 3\")\n","vectorizer = CountVectorizer(ngram_range=(3,3))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","\n","print (\"\\nn-grammes de caractères de taille 1 et 2\")\n","vectorizer = CountVectorizer(analyzer='char',ngram_range=(1,2))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLKxuBuBp9qL"},"source":["**min_df et max_df**  \n","\n","*min_df* ignore les termes qui ont une fréquence de document (présence en % de documents) strictement inférieure au seuil donné. Par exemple, *min_df = 0,55* exige qu'un terme apparaisse dans 55% des documents pour être considéré comme faisant partie du vocabulaire. *max_df* à l'inverse ignore les termes qui sont supérieurs au seuil. Il est utilisé, par exemple, pour éliminer les termes trop fréquents."]},{"cell_type":"code","metadata":{"id":"LoDlQomyp9qL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611163931542,"user_tz":-60,"elapsed":397878,"user":{"displayName":"Pascal Poncelet","photoUrl":"","userId":"12593690340642999315"}},"outputId":"4cadcbe5-7fac-4040-e0c7-0e6bc061805a"},"source":["texte = [\"This is an example of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","\n","print (\"Ne conserver que les termes qui apparaissent dans au moins 50% des documents avec min_df=0.5\")\n","vectorizer = CountVectorizer(min_df=0.5)\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","print (\"Ne conserver que les termes qui sont inférieurs à 50% des documents avec max_df=0.5\")\n","vectorizer = CountVectorizer(max_df=0.5)\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ne conserver que les termes qui apparaissent dans au moins 50% des documents avec min_df=0.5\n","['countvectorizer', 'example', 'is', 'of', 'this']\n","Ne conserver que les termes qui sont inférieurs à 50% des documents avec max_df=0.5\n","['an', 'another', 'creating', 'for', 'or', 'parameters', 'vector', 'with', 'without']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kMXuJJjip9qM"},"source":["**max_features**  \n","*max_features* permet de préciser la taille de sortie du vecteur, i.e. le nombre de termes à conserver."]},{"cell_type":"code","metadata":{"id":"xZEsW4HYp9qM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611163931542,"user_tz":-60,"elapsed":397856,"user":{"displayName":"Pascal Poncelet","photoUrl":"","userId":"12593690340642999315"}},"outputId":"56009cb3-9acd-46ce-dc3d-31d025f61fb3"},"source":["texte = [\"This is an example of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","       \"with or without parameters\"]\n","\n","print (\"Ne conserver que 8 features pour le vocabulaire\")\n","vectorizer = CountVectorizer(max_features=8)\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n","print (\"Pas de contraintes sur la taille du vocabulaire\")\n","vectorizer = CountVectorizer()\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","print (vectorizer.get_feature_names())\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ne conserver que 8 features pour le vocabulaire\n","['an', 'another', 'countvectorizer', 'creating', 'example', 'is', 'of', 'this']\n","Pas de contraintes sur la taille du vocabulaire\n","['an', 'another', 'countvectorizer', 'creating', 'example', 'for', 'is', 'of', 'or', 'parameters', 'this', 'vector', 'with', 'without']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"akvFe0qt3pug"},"source":["**Remarque :** l'inconvénient de CountVectorizer est qu'il génère des matrices qui sont creuses, i.e. il y a beaucoup de zéros. L'exemple suivant illustre le contenu de la matrice précédente où le bleu foncé indique qu'il y a une valeur et le bleu claire indique un zéro."]},{"cell_type":"code","metadata":{"id":"7IpGw_m44t1G"},"source":["sns.heatmap(X.todense(), cmap=\"Blues\", vmin=0, vmax=1).set_title('Matrice Creuse pour CountVectorizer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_7aJq23ONX0"},"source":["### **L'approche via TF_IDF** "]},{"cell_type":"markdown","metadata":{"id":"nUh6tqgkp9qM"},"source":["\n","\n","Le but de l'utilisation de tf-idf est de réduire l'impact des termes qui apparaissent très fréquemment dans un corpus donné et qui sont donc moins informatifs que les autres termes dans le corpus d'apprentissage. \n","\n","CountVectorizer, en prenant en compte l'occurrence des mots, est souvent trop limité. Une alternative est d'utiliser la \n","mesure TF-IDF (Term Frequency – Inverse Document) qui a pour but de réduire l'impact des termes qui apparaissent très fréquemment dans un corpus donné :  \n","\n","$\n"," tf$-$idf(d, t) = tf(t) * idf(d, t)\n","$  \n","\n","où $tf(t)$= la fréquence du terme, i.e. le nombre de fois où le terme apparaît dans le document  \n","et $idf(d, t)$ = la fréquence du document, i.e. le nombre de documents 'd' qui contiennent le terme 't'. \n","\n","Le principe est le même que pour CountVectorizer, cette opération se fait par :\n","1. Création d'une instance de la classe TfidfVectorizer.\n","1. Appel de la fonction fit() pour apprendre le vocabulaire.\n","1. Appel de la fonction transform() sur un ou plusieurs documents afin de les encoder dans le vecteur.  \n","\n","Les paramètres sont assez similaires à ceux de CountVectorizer, voir https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","\n","**Remarque :** Il est possible, si CountVectorizer a déjà été utilisé, de le faire suivre par TfidfTransformer pour simplement mettre à jour les valeurs."]},{"cell_type":"code","metadata":{"id":"ouwrsTmIp9qM"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","texte = [\"This is an example of TfidfVectorizer for creating a vector\",\n","        \"This is another example of TfidfVectorizer\",\n","        \"with or without parameters\"]\n","\n","print (\"Application de TfidfVectorizer :\")\n","vectorizer = TfidfVectorizer()\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8OlaY23p9qN"},"source":["Il est possible d'obtenir l'idf de chaque terme du vocabulaire via l'attribut *idf_* :"]},{"cell_type":"code","metadata":{"id":"BToMKkHtp9qN"},"source":["print (\"Affichage de l'idf de chaque terme du vocabulaire : \");\n","\n","print(dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eV-fbRbrp9qN"},"source":["Un exemple combinant différents attributs :"]},{"cell_type":"code","metadata":{"id":"hKXgeniVp9qN"},"source":["texte = [\"This is an example of TfidfVectorizer for creating a vector\",\n","        \"This is another example of TfidfVectorizer\",\n","        \"with or without parameters\"]\n","\n","print (\"Application de TfidfVectorizer avec ngram_range=(1,2) et suppression des stopwords :\")\n","vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1,2))\n","# fit et transform en une opération\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjE3YjQb6QZ4"},"source":["**Remarque :** l'un des gros avantages de TfidfVectorizer par rapport à CountVectorizer est qu'il génère des matrices moins creuses."]},{"cell_type":"markdown","metadata":{"id":"8gs6p0bTp9qO"},"source":["<font color=red>Exercice :</font> CountVectorizer et TfidfVectorizer ne possèdent pas de dictionnaire de stop words français. Cependant nous avons vu qu'il était possible de passer comme paramètre une liste de stopwords à supprimer.  \n","Télécharger le fichier : StopWordsFrench.csv et sauvegarder le sur votre répertoire courant.  \n","\n","Pour cela vous pouvez utiliser directement la commande :  \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0cQKQSYVp9qO"},"source":["!wget https://www.lirmm.fr/~poncelet/EGC2021/StopWordsFrench.csv  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JJ_bJk4p9qO"},"source":["Cette liste a été obtenue à partir du site : https://referencement-gratuit.and-co.ch/download/liste-stop-words-francais.txt\n","\n","Compléter la cellule suivante de manière à créer un pipeline qui élimine les stopwords français et détermine des n-grammes d'intervalle 1 à 2, que les features ne soient pas convertis en minuscule et qu'au final le nombre de features soit égal à 15. Il faut tester à la fois pour CountVectorizer et TfidfVectorizer. "]},{"cell_type":"code","metadata":{"id":"Y2PrCEhtp9qO"},"source":["list_french_stopwords=pd.read_csv(\"StopWordsFrench.csv\", sep=',',index_col = 0)\n","# conversion en liste\n","list_french_stopwords=list_french_stopwords.values.tolist()\n","\n","\n","texte = [\"Au clair de la lune\",\n","         \"mon ami Pierrot\",\n","        \"Prête-moi ta plume\",\n","        \"Pour écrire un mot\",\n","        \"Ma chandelle est morte\"\n","        \"Je n'ai plus de feu\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMsPfBqSp9qP"},"source":["<font color=blue>Solution :</font>"]},{"cell_type":"code","metadata":{"id":"f_Pmu5w7SxQ9"},"source":["# Pour CountVectorizer\n","vectorizer = CountVectorizer(lowercase=False,stop_words=list_french_stopwords,ngram_range=(1,2),max_features=15)\n","# fit et transform en une operation\n","X = vectorizer.fit_transform(texte)\n","\n","# creation du dataframe pour affichage\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)\n","sns.heatmap(X.todense(), cmap='Blues', vmin=0, vmax=1).set_title('Matrice pour CountVectorizer')\n","\n","# Pour TF-IDF\n","vectorizer = TfidfVectorizer(lowercase=False,stop_words=list_french_stopwords,ngram_range=(1,2),max_features=15)\n","X = vectorizer.fit_transform(texte)\n","\n","\n","df = pd.DataFrame(\n","    data=vectorizer.transform(texte).toarray(),\n","    columns=vectorizer.get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBUvJ-bZOaFg"},"source":["## **Prise en compte des prétraitements avant transformation**  \n"]},{"cell_type":"markdown","metadata":{"id":"AzwriRAUp9qQ"},"source":["\n","Précédemment nous avons vu qu'il était possible d'appliquer de très nombreux pré-traitements sur les documents. Même si CountVectorizer et TfidfVectorizer offrent certaines fonctionnalités, ces dernières ne sont pas forcément suffisantes. Dans cette section, nous présentons comment les pipelines peuvent être utilisés pour mettre en place une chaîne de traitement qui pré-traite les données pour les convertir en vecteurs.  \n","\n","Considérons la fonction suivante qui effectue un certain nombre de pré-traitements sur un seul document. Par défaut, les paramètres sont à False pour effectuer les pré-traitements. Pour l'activer un pré-traitement, il suffit de mettre le paramètre à True. \n"]},{"cell_type":"code","metadata":{"id":"BlUDgP0qp9qR"},"source":["import re\n","import string\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer \n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = set(stopwords.words('english')) \n","\n","def MyCleanText(X, \n","               lowercase=False, # mettre en minuscule\n","               removestopwords=False, # supprimer les stopwords\n","               removedigit=False, # supprimer les nombres  \n","               getstemmer=False, # conserver la racine des termes\n","               getlemmatisation=False # lematisation des termes \n","              ):\n","    \n","    sentence=str(X)\n","\n","    # suppression des caractères spéciaux\n","    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n","    # suppression de tous les caractères uniques\n","    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n","    # substitution des espaces multiples par un seul espace\n","    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n","\n","    # decoupage en mots\n","    tokens = word_tokenize(sentence)\n","    if lowercase:\n","          tokens = [token.lower() for token in tokens]\n","\n","    # suppression ponctuation\n","    table = str.maketrans('', '', string.punctuation)\n","    words = [token.translate(table) for token in tokens]\n","\n","    # suppression des tokens non alphabetique ou numerique\n","    words = [word for word in words if word.isalnum()]\n","    \n","    # suppression des tokens numerique\n","    if removedigit:\n","        words = [word for word in words if not word.isdigit()]\n","\n","    # suppression des stopwords\n","    if removestopwords:\n","        words = [word for word in words if not word in stop_words]\n","\n","    # lemmatisation\n","    if getlemmatisation:\n","        lemmatizer=WordNetLemmatizer()\n","        words = [lemmatizer.lemmatize(word)for word in words]\n","        \n","\n","    # racinisation\n","    if getstemmer:\n","        ps = PorterStemmer()\n","        words=[ps.stem(word) for word in words]\n","        \n","    sentence= ' '.join(words)\n","  \n","    return sentence   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlnIo0fWp9qR"},"source":["L'exemple suivant illustre 3 cas d'utilisation : "]},{"cell_type":"code","metadata":{"id":"a1mF7iKwp9qR"},"source":["texte = \"\"\"This is an example of using the Function MyCleanText before creating a vector created, \\\n","          this text has some problems like 1 c or even numbers like 13 and we have corpora\"\"\"\n","\n","print (\"Texte d'origine :\\n\", texte,'\\n')\n","print ('Utilisation de MyCleanText avec les paramètres par défaut (nettoyage des caractères spéciaux, des caractères uniques etc)')\n","print (MyCleanText(texte),'\\n')\n","\n","print ('Utilisation de MyCleanText avec convertion en minuscule, en prenant les racines, en supprimant les nombres')\n","print (MyCleanText(texte,lowercase=True,getstemmer=True, removedigit=True),'\\n')\n","\n","print ('Utilisation de MyCleanText avec convertion en minuscule et en mettant sous la forme de lemmes')\n","print (MyCleanText(texte,lowercase=True,getlemmatisation=True),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EegSwGbOp9qR"},"source":["**Les estimateurs et Transformer**  \n","\n","Scikit learn propose une interface Transformer qui est un type spécial d'estimateur qui crée un nouvel ensemble de données à partir d'un ancien en fonction de règles apprises lors de l'appel à la fonction *fit*. Il existe de très nombreux Transformer dans Scikit-Learn pour normaliser, mettre à l'échelle, gérer les valeurs manquantes, réduire les dimensions, etc. \n","De nombreuses informations sur les estimateurs proposés et leurs utilisations sont disponibles ici : https://scikit-learn.org/stable/developers/develop.html  \n","\n","L'interface de base pour un Transformer est la suivante : "]},{"cell_type":"markdown","metadata":{"id":"WIsMaxNVp9qS"},"source":["from sklearn.base import TransformerMixin\n","\n","class Transfomer(BaseEstimator, TransformerMixin):\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Apprendre comme transformer les données en fonction des données d'entrées X.\n","        \"\"\"\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transformer X dans un nouveau jeu de données Xprime et le retourner.\n","        \"\"\"\n","        return Xprime"]},{"cell_type":"markdown","metadata":{"id":"BFOz4kCCp9qS"},"source":["où via la méthode Transformer.transform nous pouvons transformer les données initiales.    \n","\n","Nous pouvons par exemple construire la classe *TextNormalizer* qui effectue les pré-traitements sur les données (suppression stopwords, récupération des racines, etc.) définis dans la fonction MyCleanText "]},{"cell_type":"code","metadata":{"id":"L6cu12S2p9qS"},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class TextNormalizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, \n","                 removestopwords=False, # suppression des stopwords\n","                 lowercase=False,# passage en minuscule\n","                 removedigit=False, # supprimer les nombres  \n","                 getstemmer=False,# racinisation des termes \n","                 getlemmatisation=False # lemmatisation des termes  \n","                ):\n","        \n","        self.lowercase=lowercase\n","        self.getstemmer=getstemmer\n","        self.removestopwords=removestopwords\n","        self.getlemmatisation=getlemmatisation\n","        self.removedigit=removedigit\n","\n","    def transform(self, X, **transform_params):\n","        # Nettoyage du texte\n","        X=X.copy() # pour conserver le fichier d'origine\n","        return [MyCleanText(text,lowercase=self.lowercase,\n","                            getstemmer=self.getstemmer,\n","                            removestopwords=self.removestopwords,\n","                            getlemmatisation=self.getlemmatisation,\n","                            removedigit=self.removedigit) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","    \n","    def fit_transform(self, X, y=None, **fit_params):\n","        return self.fit(X).transform(X)\n","\n","    def get_params(self, deep=True):\n","        return {\n","            'lowercase':self.lowercase,\n","            'getstemmer':self.getstemmer,\n","            'removestopwords':self.removestopwords,\n","            'getlemmatisation':self.getlemmatisation,\n","            'removedigit':self.removedigit\n","        }    \n","    \n","    def set_params (self, **parameters):\n","        for parameter, value in parameters.items():\n","            setattr(self,parameter,value)\n","        return self    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-HZpDzrFXg5"},"source":["Il est donc maintenant possible d'enchaîner des pré-traitement et l'application d'un tf-idf."]},{"cell_type":"code","metadata":{"id":"XsVGJ-j4Ff-D"},"source":["texte = [\"This is an example of TfidfVectorizer for creating a vector\",\n","        \"This is another example of TfidfVectorizer\",\n","        \"but before we apply a preprocessing\"]\n","\n","print (\"texte avant \",texte)\n","# il suffit de créer un objet de la classe TextNormalizer\n","text_normalizer=TextNormalizer(lowercase=True)  \n","# d'appliquer fit.transform pour appliquer les pré-traitements\n","text_cleaned=text_normalizer.fit_transform(texte)\n","print (\"texte après application des pré-traitements\")\n","print (text_cleaned)     \n","\n","# pour l'enchainer avec un tf-idf : \n","tfidf=TfidfVectorizer(ngram_range=(2, 2))\n","vector_tfidf=tfidf.fit_transform(text_cleaned)\n","print (\"texte transformé en vecteur tf-idf\")\n","print (vector_tfidf)\n","\n","# le vecteur peut par la suite être transformé en matrice : \n","print (\"transformation du vecteur en matrice\")\n","vector_tfidf.toarray()\n","\n","# notons que cette matrice pourra être à l'entrée d'un classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jUR7ETmp9qS"},"source":["La généralisation du principe précédent via un pipeline se fait alors simplement : "]},{"cell_type":"code","metadata":{"id":"gsNS4ps-p9qT"},"source":["from sklearn.pipeline import Pipeline\n","\n","texte = [\"This is an example of TextNormalizer then TfidfVectorizer for creating a vector\",\n","        \"This is not another example of CountVectorizer\",\n","        \"with or without parameters. Rather is a mainly a pipeline with more or less default parameters\"]\n","\n","pipe = Pipeline([(\"cleaner\", TextNormalizer()),\n","                 (\"count_vectorizer\", TfidfVectorizer(ngram_range=(2, 2)))])\n","pipe.fit(texte)\n","pipe.transform(texte)\n","\n","\n","# creation du dataframe pour affichage\n","# il est possible d'accèder à une étape du pipeline en spécifiant le nom (e.g. pipe['cleaner'])\n","# dans le dataframe on récupère les différents features comme nom de colonnes\n","df = pd.DataFrame(\n","    data=pipe.transform(texte).toarray(),\n","    columns=pipe['count_vectorizer'].get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VQQPmX0Xp9qT"},"source":["**Attention :** pour rappel CountVectorizer ou TfidfVectorizer mettent en minuscule par défaut. \n"]},{"cell_type":"code","metadata":{"id":"6t24852Ap9qT"},"source":["texte = [\"This is an example of CountVectorizer for creating a vector\",\n","        \"This is another example of CountVectorizer\",\n","        \"with or without parameters\"]\n","\n","pipe = Pipeline([(\"cleaner\", TextNormalizer(removestopwords=False,lowercase=False)),\n","                 (\"count_vectorizer\", TfidfVectorizer(lowercase=False))])\n","pipe.fit(texte)\n","pipe.transform(texte)\n","\n","\n","df = pd.DataFrame(\n","    data=pipe.transform(texte).toarray(),\n","    columns=pipe['count_vectorizer'].get_feature_names()\n",")\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXwq2BYYp9qT"},"source":["## **Exemple de classification**"]},{"cell_type":"markdown","metadata":{"id":"nA9b_rcRp9qT"},"source":["Maintenant que nous savons pré-traiter les données et construire des vecteurs nous pouvons passer à l'étape de classification. \n","Le jeu de donnée que nous allons utiliser est tiré de la base de l'UCI : https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n","et a été créée par : \"From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\".  \n","\n","Il contient des phrases d'avis de trois sites différents (Yeld, Amazon et Imbd) et contient pour chacun de ces sites 500 avis positifs (valeur = 1) et 500 avis négatif (valeur = 0).\n","Le site : https://archive.ics.uci.edu/ml/machine-learning-databases/00331/\n","propose 3 fichiers de textes bruts nommés amazon_cells_labelled.txt, imdb_labelled.txt et yelp_labelled.txt. \n","\n","Il est possible de télécharger une version regroupant les trois fichiers en un seul fichier ici : https://www.lirmm.fr/~poncelet/ReviewsLabelled.csv  \n","\n","Pour le télécharger depuis le notebook vous pouvez exécuter la cellule suivante : \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"y8gg6jw-p9qT"},"source":["!wget https://www.lirmm.fr/~poncelet/EGC2021/ReviewsLabelled.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fy4ye6Mqp9qU"},"source":["Lecture du fichier "]},{"cell_type":"code","metadata":{"id":"XNmZ83PVp9qU"},"source":["df = pd.read_csv(\"ReviewsLabelled.csv\", names=['sentence','sentiment','source'], header=0,sep='\\t', encoding='utf8')\n","\n","print (\"les 10 premières lignes du fichier :\")\n","display(df[0:10])\n","print (\"la taille du fichier : \", df.shape)\n","print (\"le nombre d'avis différents : \\n\",df['sentiment'].value_counts(),'\\n')\n","print (\"Un exemple d'avis \\n\",df['sentence'][0],'\\n')\n","\n","# selection des données\n","X=df.sentence\n","y=df.sentiment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1C7zCNLp9qU"},"source":["## **Quel est le meilleur pré-traitement et la meilleure représentation de vecteur ?**"]},{"cell_type":"markdown","metadata":{"id":"0-uXJ-P2p9qU"},"source":["Précédemment nous avons vu comment pré-traiter les données et comment transformer les données en vecteur. Par exemple, appliquons le pipeline sur les différentes phrases du corpus de données pour les transformer : \n"]},{"cell_type":"code","metadata":{"id":"pmRKUVEmp9qU"},"source":["# création du pipeline\n","pipe = Pipeline([(\"cleaner\", TextNormalizer()),\n","                 (\"count_vectorizer\", TfidfVectorizer())])\n","pipe.fit(X)\n","pipe.transform(X)\n","\n","# creation du dataframe pour affichage\n","df_pipe = pd.DataFrame(\n","    data=pipe.transform(X).toarray(),\n","    columns=pipe['count_vectorizer'].get_feature_names()\n",")\n","\n","display(df_pipe)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUSD1f7qp9qU"},"source":["**Un premier essai simple de classification**  \n","\n","L'objectif ici est d'utiliser un premier classifier et d'étudier quels sont les pré-traitements et vecteurs les plus efficaces pour ce dernier. Pour commencer nous prendrons SVM qui obtient souvent de bons résultats sur les données textuelles. \n","\n","Nous pouvons donc, pour simplifier, créer un jeu d'apprentissage et un jeu de test et évaluer le résultat d'un classifieur SVM placé dans un pipeline.\n"]},{"cell_type":"code","metadata":{"id":"knr43BQ8p9qU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611163939139,"user_tz":-60,"elapsed":405125,"user":{"displayName":"Pascal Poncelet","photoUrl":"","userId":"12593690340642999315"}},"outputId":"3ded6319-908b-4344-f185-4dbd80acbd40"},"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Création d'un jeu d'apprentissage et de test\n","trainsize=0.7 # 70% pour le jeu d'apprentissage, il reste 30% du jeu de données pour le test\n","\n","testsize= 0.3\n","seed=30\n","X_train,X_test,y_train,y_test=train_test_split(X, \n","                                               y, \n","                                               train_size=trainsize, \n","                                               random_state=seed,\n","                                               test_size=testsize)\n","\n","# création du pipeline en ajoutant le classifier\n","pipe = Pipeline([(\"cleaner\", TextNormalizer()),\n","                 (\"count_vectorizer\", CountVectorizer()),\n","                 (\"SVM\", SVC())])\n","pipe.fit(X_train,y_train)\n","\n","print(\"pipeline créé\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pipeline créé\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bw8YtEqVp9qU"},"source":["Prediction pour évaluer la qualité du modèle appris : "]},{"cell_type":"code","metadata":{"id":"NPZlnEv7p9qU"},"source":["#from sklearn import metrics\n","\n","\n","y_pred = pipe.predict(X_test)\n","\n","MyshowAllScores(y_test,y_pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SN95-o0p9qU"},"source":["L'objectif à présent est de pouvoir tester plusieurs pré-traitement et vectorisation afin de déterminer ceux qui amènent à la meilleure classification. Pour cela il suffit de créer autant de pipeline que l'on souhaite tester."]},{"cell_type":"code","metadata":{"id":"KLSE965ep9qV"},"source":["# pipeline de l'utilisation de CountVectorizer sur le texte presque sans traitement\n","CV_brut = Pipeline([(\"cleaner\", TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=True,removedigit=True)), \n","                    (\"count_vectorizer\", CountVectorizer(lowercase=False)),\n","                    (\"SVM\", SVC())])\n","\n","# pipeline de l'utilisation de TfidfVectorizer sur le texte presque sans traitement\n","TFIDF_brut = Pipeline ([(\"cleaner\", TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=True,removedigit=True)), \n","                    (\"count_vectorizer\", TfidfVectorizer(lowercase=False)),\n","                    (\"SVM\", SVC())])\n","\n","# Liste de tous les modèles à tester\n","all_models = [\n","    (\"CV_brut\", CV_brut),\n","    (\"TFIDF_brut\", TFIDF_brut)\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0Fgj7ZDp9qV"},"source":["L'évaluation se fait en utilisant cross_val_score qui effectue une cross validation et évalue le modèle."]},{"cell_type":"code","metadata":{"id":"aXEQ7bSQp9qV"},"source":["from sklearn.model_selection import cross_val_score\n","\n","\n","# Evaluation des différents pipelines\n","unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n","scores = sorted(unsorted_scores, key=lambda x: -x[1])\n","\n","\n","print (tabulate(scores, floatfmt=\".3f\", headers=(\"Pipeline\", 'Score')))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtQRVBOVp9qV"},"source":["<font color=red>Exercice :</font> En vous inspirant du code précédent, proposer la combinaison pré-traitement, vectorisation qui permette d'obtenir le meilleur score de classification. "]},{"cell_type":"code","metadata":{"id":"rYPqgetYnqHn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDwiGLshp9qV"},"source":["<font color=blue> Solution :</font>\n","  "]},{"cell_type":"code","metadata":{"id":"lXlzs8PexeRP"},"source":["# le plus simple est de faire un test sur differents pipelines.  \n","# pipeline de l'utilisation de CountVectorizer sur le texte avec differents pre-traitements\n","CV_brut = Pipeline([('cleaner', TextNormalizer()), \n","                    ('count_vectorizer', CountVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","CV_lowcase = Pipeline([('cleaner', TextNormalizer(removestopwords=False,lowercase=True,\n","                                               getstemmer=False,removedigit=False)), \n","                    ('count_vectorizer', CountVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","CV_lowStop = Pipeline([('cleaner', TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=False,removedigit=False)), \n","                    ('count_vectorizer', CountVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","\n","CV_lowStopstem = Pipeline([('cleaner', TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=True,removedigit=False)), \n","                    ('count_vectorizer', CountVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","\n","# pipeline de l'utilisation de TfidfVectorizer avec differents pre-traitements\n","TFIDF_brut = Pipeline ([('cleaner', TextNormalizer()), \n","                    ('tfidf_vectorizer', TfidfVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","\n","TFIDF_lowcase = Pipeline([('cleaner', TextNormalizer(removestopwords=False,lowercase=True,\n","                                               getstemmer=False,removedigit=False)), \n","                    ('tfidf_vectorizer', TfidfVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","TFIDF_lowStop = Pipeline([('cleaner', TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=False,removedigit=False)), \n","                    ('tfidf_vectorizer', TfidfVectorizer(lowercase=False)),\n","                    ('svm', SVC())])\n","\n","TFIDF_lowStopstem = Pipeline([('cleaner', TextNormalizer(removestopwords=True,lowercase=True,\n","                                               getstemmer=True,removedigit=False)), \n","                    ('tfidf_vectorizer', TfidfVectorizer(lowercase=False)),\n","                    (\"svm\", SVC())])\n","\n","\n","# Liste de tous les modeles à tester\n","all_models = [\n","    (\"CV_brut\", CV_brut),\n","    (\"CV_lowcase\", CV_lowcase),\n","    (\"CV_lowStop\", CV_lowStop),\n","    (\"CV_lowStopstem\",CV_lowStopstem),\n","    (\"TFIDF_lowcase\", TFIDF_lowcase),\n","    (\"TFIDF_lowStop\", TFIDF_lowStop),\n","    (\"TFIDF_lowStopstem\",TFIDF_lowStopstem),\n","    (\"TFIDF_brut\", TFIDF_brut)\n","]\n","\n","\n","# Evaluation des differents pipelines\n","print (\"Evaluation des différentes configurations : \")\n","unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n","scores = sorted(unsorted_scores, key=lambda x: -x[1])\n","\n","\n","print (tabulate(scores, floatfmt='.4f', headers=('Pipeline', 'Score')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPSeepIRPFNc"},"source":["## **Evaluation de différents classifieurs**  "]},{"cell_type":"markdown","metadata":{"id":"F9lL_XkJp9qV"},"source":["\n","\n","Dans cette section, nous évaluons différents classifieurs pour voir lequel est le plus performant. Comme nous appliquons pour chaque classifier les mêmes pré-traitements (appel de TextNormalizer sans paramètres) et l'obtention de la matrice pour tf-idf. Plutôt que de faire des pipelines et de relancer cette étape pour chaque classifier nous la réalisons en premier. Puis nous testons les différents classifiers via une cross validation.  "]},{"cell_type":"code","metadata":{"id":"YGKkSSwwp9qV"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# creation du tableau des différents classifieur \n","\n","\n","models = []\n","models.append(('MultinomialNB',MultinomialNB()))\n","models.append(('LR', LogisticRegression(solver='lbfgs')))\n","models.append(('KNN', KNeighborsClassifier()))\n","models.append(('CART', DecisionTreeClassifier()))\n","models.append(('RF', RandomForestClassifier()))\n","models.append(('SVM', SVC()))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YByH2q3Cp9qV"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","\n","score = 'accuracy'\n","seed = 7        \n","allresults = []\n","results = []\n","names = []\n","\n","X=df.sentence\n","y=df.sentiment\n","\n","# Nous appliquons les pré-traitements sur X\n","\n","text_normalizer=TextNormalizer()  \n","# appliquer fit.transform pour réaliser les pré-traitements sur X\n","X_cleaned=text_normalizer.fit_transform(X)\n","\n","# pour l'enchainer avec un tf-idf et obtenir une matrice\n","tfidf=TfidfVectorizer()\n","features=tfidf.fit_transform(X_cleaned).toarray()\n","\n","# attention ici il faut passer features dans cross_val_score plutôt que X\n","    \n","for name,model in models:\n","    # cross validation en 10 fois\n","    kfold = KFold(n_splits=10, random_state=seed)\n","    \n","    print (\"Evaluation de \",name)\n","    start_time = time.time()\n","    # application de la classification\n","    cv_results = cross_val_score(model, features, y, cv=kfold, scoring=score)\n","    \n","    # pour afficher les paramètres du modèle en cours et la taille du vecteur intermédiaire\n","    # enlever le commentaire des deux lignes suivantes \n","    #print (\"paramètre du modèle \",model.get_params(),'\\n')\n","    #print (\"taille du vecteur : \",(model.named_steps['tfidf_vectorizer'].fit_transform(X)).shape,'\\n')\n","\n","    thetime=time.time() - start_time\n","    result=Result(name,cv_results.mean(),cv_results.std(),thetime)\n","    allresults.append(result)\n","    # pour affichage\n","    results.append(cv_results)\n","    names.append(name)\n","    print(\"%s : %0.3f (%0.3f) in %0.3f s\" % (name, cv_results.mean(), cv_results.std(),thetime))         \n","    \n","allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True) \n","\n","# affichage des résultats\n","print ('\\nLe meilleur resultat : ')\n","print ('Classifier : ',allresults[0].name, \n","       ' %s : %0.3f' %(score,allresults[0].scoremean), \n","       ' (%0.3f)'%allresults[0].stdresult,  \n","       ' en %0.3f '%allresults[0].timespent,' s\\n')\n","\n","print ('Tous les résultats : \\n')\n","for result in allresults:\n","    print ('Classifier : ',result.name, \n","       ' %s : %0.3f' %(score,result.scoremean), \n","       ' (%0.3f)'%result.stdresult,  \n","       ' en %0.3f '%result.timespent,' s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGO13uhzp9qW"},"source":["import matplotlib.pyplot as plt\n","fig = plt.figure()\n","fig.suptitle('Comparaison des algorithmes')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-i5UkAsp9qW"},"source":["<font color=red>Exercice :</font> En vous inspirant du code précédent, évaluer les différents classifiers non plus par rapport à l'accuracy mais par rapport au rappel (*recall*) ou à la précision (*precision*). "]},{"cell_type":"code","metadata":{"id":"ErqotNVbnywJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHPCZfTtp9qW"},"source":["<font color=blue>Solution :</font> "]},{"cell_type":"code","metadata":{"id":"hzDTL1XDyLEZ"},"source":["# il suffit de remplacer dans le code score ='recall' ou score='precision'.\n","print (\"Pour le rappel : \")\n","score = 'recall'       \n","allresults = []\n","    \n","for name,model in models:\n","    kfold = KFold(n_splits=10, random_state=seed)\n","    print (\"Evaluate \",name, 'pour ',score)\n","    start_time = time.time()\n","    # application de la classification\n","    cv_results = cross_val_score(model, features, y, cv=kfold, scoring=score)\n","    \n","    thetime=time.time() - start_time\n","    result=Result(name,cv_results.mean(),cv_results.std(),thetime)\n","    allresults.append(result)\n","    print(\"%s : %0.3f (%0.3f) in %0.3f s\" % (name, cv_results.mean(), cv_results.std(),thetime))         \n","    \n","allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True) \n","\n","print ('Le meilleur resultat : ')\n","print ('Classifier : ',allresults[0].name, \n","       ' %s : %0.3f' %(score,allresults[0].scoremean), \n","       ' (%0.3f)'%allresults[0].stdresult,  \n","       ' en %0.3f '%allresults[0].timespent,' s')\n","\n","print ()\n","print ('Tous les résultats : ')\n","for result in allresults:\n","    print ('Classifier : ',result.name, \n","       ' %s : %0.3f' %(score,result.scoremean), \n","       ' (%0.3f)'%result.stdresult,  \n","       ' en %0.3f '%result.timespent,' s')\n","    \n","    \n","print ()    \n","print (\"Pour la precision : \")\n","score = 'precision'       \n","allresults = []\n","    \n","for name,model in models:\n","    kfold = KFold(n_splits=10, random_state=seed)\n","    print (\"Evaluate \",name, 'pour ',score)\n","    start_time = time.time()\n","    # application de la classification\n","    cv_results = cross_val_score(model, features, y, cv=kfold, scoring=score)\n","    \n","    thetime=time.time() - start_time\n","    result=Result(name,cv_results.mean(),cv_results.std(),thetime)\n","    allresults.append(result)\n","    print(\"%s : %0.3f (%0.3f) in %0.3f s\" % (name, cv_results.mean(), cv_results.std(),thetime))         \n","    \n","allresults=sorted(allresults, key=lambda result: result.scoremean, reverse=True) \n","\n","print ('Le meilleur resultat : ')\n","print ('Classifier : ',allresults[0].name, \n","       ' %s : %0.3f' %(score,allresults[0].scoremean), \n","       ' (%0.3f)'%allresults[0].stdresult,  \n","       ' en %0.3f '%allresults[0].timespent,' s')\n","\n","print ('Tous les résultats : ')\n","for result in allresults:\n","    print ('Classifier : ',result.name, \n","       ' %s : %0.3f' %(score,result.scoremean), \n","       ' (%0.3f)'%result.stdresult,  \n","       ' en %0.3f '%result.timespent,' s') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yF9ku7v-p9qW"},"source":["**Remarque :** dans les code précédents les différentes opérations efffectuées se font pour chaque classifieur : l'application des pré-traitements et la vectorisation avec TfidfVectorizer. Il aurait été bien sûr possible d'effectuer les prétraitements et vectorisation au préalable et d'appliquer uniquement les classifiers sur les données transformées.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AKqs36REPNXT"},"source":["## **Recherche des hyperparamètres**"]},{"cell_type":"markdown","metadata":{"id":"Ptjf-gmfp9qW"},"source":["\n","\n","Nous avons vu que SVM obtenait de bons résultats par rapport aux autres. Dans cette section nous étudions les hyperparamètres. Précédemment, nous avons vu après les avoir testé quels étaient également les meilleurs paramètres pour les pré-traitement et la vectorisation. Nous montrons aussi qu'il est possible de rechercher en même temps et de manière automatique quels sont les meilleurs paramètres."]},{"cell_type":"markdown","metadata":{"id":"vajjNShPp9qW"},"source":["SVM dispose de différents hyperparamètres qui peuvent être pris en compte dont les principaux sont :  \n","1. Le Noyaux (*kernel*) dont La fonction principale est de prendre un espace d'entrée de faible dimension et de le transformer en un espace de dimension supérieure. Il est surtout utile dans les problèmes de séparation non linéaire. Scikit learn propose les noyaux suivants : 'linear','poly','rbf','sigmoid'.\n","1. *C* (*regularization*) qui est le paramètre de pénalité, qui représente une mauvaise classification ou un terme d'erreur. Le terme de classification erronée ou d'erreur indique à l'optimisation SVM le niveau d'erreur supportable. Il permet de contrôler le compromis entre la frontière de décision un élément mal classé. En général plus C est grand mieux il classera les données mais cela entraîne aussi des fois du supapprentissage (*overfitting*). Inversement un C trop petit peut entraîner du sous-apprentissage (*underfitting*).\n","1. *Gamma* qui définit jusqu'où l'influence d'un seul exemple d'entraînement peut aller, avec des valeurs faibles signifiant «loin» et des valeurs élevées signifiant «proche». Lorsque Gamma est élevé, les points proches auront une forte influence; un gamma faible signifie par contre que des points éloignés doivent également être pris en compte pour obtenir la limite de décision.  \n","\n","Vous pourrez trouver plus d'informations sur les hyperparamètres de SVM sous scikit learn ici : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html  \n","\n","Nous pouvons donc créer, à présent, un pipeline et spécifier les hyperparamètres à tester via GridSearchCV. Dans un premier temps, nous considérons un pipeline composé uniquent de la vectorisation et du classifier.  \n","\n","**Remarque :** GridSearchCV effectue par défaut une cross validation (*cv*) avec une valeur par défaut de 5. Par contre étant donné qu'il fonctionne sur l'ensemble des données, il n'est pas possible par la suite de pouvoir obtenir d'autres mesures que celle qui est réalisée (notamment la matrice de confusion). Pour cela il est conseillé de couper le jeu de données en apprentissage (sur lequel sera appliqué le GridSearchCV) et un jeu de test avec 90/10."]},{"cell_type":"code","metadata":{"id":"f6kNqkZop9qW"},"source":["# Création du jeu d'apprentissage et de test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"qwYT-cRqp9qX"},"source":["from sklearn.model_selection import GridSearchCV\n","\n","\n","pipeline=Pipeline([(\"tfidf\", TfidfVectorizer()),\n","                   ('svm', SVC())])\n","\n","# creation des différents paramètres à tester pour SVM\n","# Attention dans le pipeline le nom pour le classifier SVM est : svm même si l'algorithme s'appelle SVC\n","# pour le référencer il faut utiliser le nom utilisé, i.e. svm, puis deux caractères soulignés\n","# et enfin le nom du paramètre\n","parameters = { \n","     'svm__C': [0.001, 0.01, 0.1, 1, 10], \n","    'svm__gamma' : [0.001, 0.01, 0.1, 1], \n","    'svm__kernel': ['linear','rbf','poly','sigmoid']}\n","    \n","\n","score='accuracy'\n","\n","# Application de gridsearchcv, n_jobs=-1 permet de pouvoir utiliser plusieurs CPU s'ils sont disponibles\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1,scoring=score)\n","\n","print(\"Application de gridsearch ...\")\n","print(\"pipeline :\", [name for name, _ in pipeline.steps])\n","print(\"parameters :\")\n","print(parameters)\n","start_time = time.time()\n","grid_search.fit(X_train, y_train)\n","print(\"réalisé en  %0.3f s\" % (time.time() - start_time))\n","print(\"Meilleur résultat : %0.3f\" % grid_search.best_score_)\n","\n","# autres mesures et matrice de confusion\n","y_pred = grid_search.predict(X_test)\n","MyshowAllScores(y_test,y_pred)\n","\n","\n","print(\"Ensemble des meilleurs paramètres :\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","        \n","# Affichage des premiers résultats du gridsearch\n","df_results=pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),\n","           pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], \n","                        columns=[score])],axis=1).sort_values(score,ascending=False)\n","print (\"\\nLes premiers résultats : \\n\",df_results.head()) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uibEZLo0p9qX"},"source":["**Remarque :** dans GridSearchCV, le paramètre utilisé *n_jobs=-1* permet de pouvoir réaliser des traitements en parallèle en utilisant tous les CPU de la machine. "]},{"cell_type":"markdown","metadata":{"id":"HeThus0jp9qX"},"source":["Et maintenant pourquoi ne pas rechercher également les meilleurs paramètres aussi bien pour le pré-traitement des données que pour la vectorisation et le classifier.  \n","\n","**Attention :** cette recherche peut bien entendu être très longue aussi dans le code suivant nous ne traitons que quelques paramètres et simplifions les hyperparamètres de SVM. \n","\n","Nous considérons à présent principalement les paramètres associés à TfidfVectorizer."]},{"cell_type":"code","metadata":{"id":"00zpiPfXp9qY"},"source":["pipeline=Pipeline([(\"cleaner\", TextNormalizer()),\n","                   (\"tfidf\", TfidfVectorizer()),\n","                   ('svm', SVC())])\n","\n","\n","parameters = {\n","    'tfidf__stop_words':['english',None],\n","    'tfidf__lowercase': ['True','False'], \n","     'svm__C': [1, 10], \n","    'svm__gamma' : [1], \n","    'svm__kernel': ['rbf']}\n","    \n","score='accuracy'\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1, scoring=score)\n","\n","print(\"Application de gridsearch ...\")\n","print(\"pipeline :\", [name for name, _ in pipeline.steps])\n","print(\"parameters :\")\n","print(parameters)\n","start_time = time.time()\n","grid_search.fit(X_train, y_train)\n","print(\"réalisé en  %0.3f s\" % (time.time() - start_time))\n","print(\"Meilleur résultat : %0.3f\" % grid_search.best_score_)\n","\n","# autres mesures et matrice de confusion\n","y_pred = grid_search.predict(X_test)\n","MyshowAllScores(y_test,y_pred)\n","\n","print(\"Ensemble des meilleurs paramètres :\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","        \n","# Affichage des premiers résultats du gridsearch\n","df_results=pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),\n","           pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], \n","                        columns=[score])],axis=1).sort_values(score,ascending=False)\n","print (\"\\nLes premiers résultats : \\n\",df_results.head())  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jpcYoPJp9qY"},"source":["Enfin, nous examinons l'impact de quelques paramètres notamment la racinisation associées au prétraitement."]},{"cell_type":"code","metadata":{"id":"gPXlvkTcp9qY"},"source":["pipeline=Pipeline([(\"cleaner\", TextNormalizer()),\n","                   (\"tfidf\", TfidfVectorizer()),\n","                   ('svm', SVC())])\n","\n","\n","parameters = {\n","    'cleaner__getstemmer': ['True','False'],\n","    'cleaner__removedigit':['True','False'],\n","    'cleaner__removestopwords':['True','False'],\n","    'tfidf__lowercase': ['True','False'], \n","    'svm__C': [1, 10], \n","    'svm__gamma' : [1], \n","    'svm__kernel': ['rbf']}\n","    \n","score='accuracy'\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)\n","\n","print(\"Application de gridsearch ...\")\n","print(\"pipeline :\", [name for name, _ in pipeline.steps])\n","print(\"parameters :\")\n","print(parameters)\n","start_time = time.time()\n","grid_search.fit(X_train, y_train)\n","print(\"réalisé en  %0.3f s\" % (time.time() - start_time))\n","print(\"Meilleur résultat : %0.3f\" % grid_search.best_score_)\n","\n","# autres mesures et matrice de confusion\n","y_pred = grid_search.predict(X_test)\n","MyshowAllScores(y_test,y_pred)\n","\n","\n","print(\"Ensemble des meilleurs paramètres :\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","        \n","# Affichage des premiers résultats du gridsearch\n","df_results=pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),\n","           pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], \n","                        columns=[score])],axis=1).sort_values(score,ascending=False)\n","print (\"\\nLes premiers résultats : \\n\",df_results.head()) \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDf3OxxUp9qZ"},"source":["La prochaine cellule s'intéresse à la lemmatisation et à son impact sur la classification."]},{"cell_type":"code","metadata":{"id":"ooprGklip9qZ"},"source":["pipeline=Pipeline([(\"cleaner\", TextNormalizer()),\n","                   (\"tfidf\", TfidfVectorizer()),\n","                   ('svm', SVC())])\n","\n","\n","parameters = {\n","    'cleaner__getlemmatisation': ['True','False'],\n","    'cleaner__removedigit':['True','False'],\n","    'cleaner__removestopwords':['True','False'],\n","    'tfidf__lowercase': ['True','False'], \n","    'svm__C': [1, 10], \n","    'svm__gamma' : [1], \n","    'svm__kernel': ['rbf']}\n","    \n","score='accuracy'\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)\n","\n","print(\"Application de gridsearch ...\")\n","print(\"pipeline :\", [name for name, _ in pipeline.steps])\n","print(\"parameters :\")\n","print(parameters)\n","start_time = time.time()\n","grid_search.fit(X_train, y_train)\n","print(\"réalisé en  %0.3f s\" % (time.time() - start_time))\n","print(\"Meilleur résultat : %0.3f\" % grid_search.best_score_)\n","\n","# autres mesures et matrice de confusion\n","y_pred = grid_search.predict(X_test)\n","MyshowAllScores(y_test,y_pred)\n","\n","\n","print(\"Ensemble des meilleurs paramètres :\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","        \n","# Affichage des premiers résultats du gridsearch\n","df_results=pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),\n","           pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], \n","                        columns=[score])],axis=1).sort_values(score,ascending=False)\n","print (\"\\nLes premiers résultats : \\n\",df_results.head()) \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhUfBrXxp9qZ"},"source":["<font color=red>Exercice :</font> Comparer attentivement les  dernières exécutions. Que pouvez vous en déduire sur la meilleure configuration ? Quel est l'impact des paramètres de pré-traitement et de vectorisation ? Quel pouvez vous en déduire de manière générale ? "]},{"cell_type":"markdown","metadata":{"id":"viDZ2Oyip9qZ"},"source":["<font color=blue>Solution :</font> \n"]},{"cell_type":"code","metadata":{"id":"6-JQ1xg9y-f3"},"source":["# Il s'avère que parfois les pré-traitements n'améliore pas forcément  (par exemple la lemmatisation)\n","# et que l'accuracy diminue.  Pourtant si l'on regarde la première expérimentation il s'avère qu'elle revient \n","# à traiter le texte brut sans aucun pré-traitemet et que les quelques légers pré-traitement de base \n","# suppression de caractères spéciaux, etc permettent d'améliorer très légérement les résultats (0.824 vs 0.825).  \n","# La question par contre, de manière générale, est comment améliorer ces résultats. En fait si l'on \n","# regarde plus en détail les données on peut constater que le jeu de données n'est pas très volumineux et \n","# qu'il n'y a pas forcément possibilité de mieux discriminer les avis positifs ou négatifs. \n","# Pour améliorer la classification il existe cependant de nombreuses pistes d'autres pré-traitements \n","# (e.g. ne retenir que des verbes, adjectifs souvent porteurs d'opinions et adverbe, \n","# regarder plus spécifiquement les données positives et négatives (e.g. mieux analyser les données), \n","# utiliser d'autres classifier ou d'autres types de classifiers (e.g. LR, CNN, etc.), utiliser \n","# d'autres approches de représentation des données (e.g. des embeddings que nous verrons plus tard), \n","# enrichir les données, ..... bref de très nombreuses pistes de recherche."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2cM4XiMp9qZ"},"source":["<font color=red>Exercice :</font> Dans le code ci-dessous, sélectionner un classifier et ses paramètres associés afin de tester s'il obtient de meilleurs résultats que SVM. Pour cela, il suffit de décommenter les lignes associèes au classifier que vous désirez et exécuter la cellule.  \n","Le code actuel permet de lancer la classifier LogisticRegression. Ne pas oublier de commenter les lignes associées pour tester d'autres classifiers."]},{"cell_type":"code","metadata":{"id":"jNak80uDp9qZ"},"source":["# Pour information le nombre de paramètres à tester a été défini de manière \n","# à ce que les tests ne soient pas trop longs\n","\n","pipeline=Pipeline([(\"cleaner\", TextNormalizer()),\n","                   (\"tfidf\", TfidfVectorizer()),\n","    \n","                   # pour LogisticRegression enlever le commentaire de la ligne suivante\n","                   (\"lr\", LogisticRegression()),\n","    \n","                   # pour MultinomialNB enlever le commentaire de la ligne suivante \n","                   #(\"mnb\", MultinomialNB()),\n","    \n","                   # pour RandomForestClassifier enlever le commentaire de la ligne suivante\n","                   #('rfc', RandomForestClassifier())\n","                  ]\n","                 )\n","\n","\n","parameters = {\n","    # Pour logisticRegression enlever les commentaires des 3 lignes suivantes :\n","    'lr__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n","    'lr__penalty' : ['l2'],\n","    'lr__C' : [100, 10, 1.0, 0.1, 0.01],\n","    \n","    # Pour MulinomialNaiveBayes enlever les commentaires des 2 lignes suivantes :\n","    #'mnb__alpha': np.linspace(0.5, 1.5, 6),\n","    #'mnb__fit_prior': [True, False],  \n","    \n","    # pour RandomForestClassifier enlever les commentaires des 4 lignes suivantes :\n","    #'rfc__n_estimators': [500, 1200],\n","    #'rfc__max_depth': [25, 30],\n","    #'rfc__min_samples_split': [5, 10, 15],\n","    #'rfc__min_samples_leaf' : [1, 2], \n","    }\n","     \n","    \n","score='accuracy'\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,  verbose=1,scoring=score)\n","\n","print(\"Application de gridsearch ...\")\n","print(\"pipeline :\", [name for name, _ in pipeline.steps])\n","print(\"parameters :\")\n","print(parameters)\n","start_time = time.time()\n","grid_search.fit(X, y)\n","print(\"réalisé en  %0.3f s\" % (time.time() - start_time))\n","print(\"Meilleur résultat : %0.3f\" % grid_search.best_score_)\n","\n","# matrice de confusion\n","y_pred = grid_search.predict(X_test)\n","MyshowAllScores(y_test,y_pred)\n","\n","print(\"Ensemble des meilleurs paramètres :\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","        \n","# Affichage des premiers résultats du gridsearch\n","df_results=pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),\n","           pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], \n","                        columns=[score])],axis=1).sort_values(score,ascending=False)\n","print (\"\\nLes premiers résultats : \\n\",df_results.head()) \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJzyf2mhp9qZ"},"source":["## **Sauvegarde du modèle**"]},{"cell_type":"markdown","metadata":{"id":"wcA2IzYOp9qa"},"source":["Dans cette section nous sauvegardons le modèle pour pouvoir l'utiliser ultérieurement. Pour cela, nous utilisons simplement un pipeline en utilisant les paramètres et hyperparamètres qui ont été appris précédemment. Nous l'appliquons cette fois-ci à l'ensemble du jeu de données et non plus à un sous ensemble (X_train).  "]},{"cell_type":"code","metadata":{"id":"YtwmwjcQp9qa"},"source":["\n","X=df.sentence\n","y=df.sentiment\n","\n","pipeline=Pipeline([(\"cleaner\", TextNormalizer()),\n","                   (\"tfidf\", TfidfVectorizer(lowercase='True')),\n","                   ('svm', SVC(C=1, gamma=1,kernel='rbf'))])\n","pipeline.fit(X,y)\n","filename='SentimentModel.pkl'\n","print(\"Sauvegarde du modèle dans \", filename)\n","pickle.dump(pipeline, open(filename, \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNvrzQ9Up9qa"},"source":["Maintenant qu'un modèle a été appris et sauvegardé .... il suffit de l'utiliser. Pour cela rendez vous sur le notebook **\"4 - Utiliser un modele\"** ..."]}]}